\documentclass[12pt, a4paper]{article}

% ===== ПАКЕТЫ И НАСТРОЙКИ =====
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[english, russian]{babel}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{left=30mm, right=20mm, top=20mm, bottom=20mm}
\usepackage{setspace}
\onehalfspacing
\usepackage{indentfirst}
\setlength{\parindent}{12.5mm}
\usepackage{titlesec}
\usepackage{enumitem}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=red]{hyperref}
\usepackage{cite} % ЗАМЕНА biblatex НА cite
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{babel}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{verbatim}

% ===== ШРИФТ TIMES NEW ROMAN =====
\usepackage{mathptmx}

% ===== НАСТРОЙКИ ДОКУМЕНТА =====
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyfoot[C]{\thepage}

% Настройка заголовков
\titleformat{\section}{\normalfont\Large\bfseries\centering}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

% ===== МАТЕМАТИЧЕСКИЕ КОМАНДЫ =====
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\sign}{\text{sign}}
\newcommand{\Var}{\text{Var}}

% ===== КОМАНДА ДЛЯ TIKZ ИЗОБРАЖЕНИЙ =====
\newcommand{\inputtikz}[1]{\input{tikz/#1}}

% ===== БИБЛИОГРАФИЯ =====
\bibliographystyle{gost2008} % Стиль ГОСТ для BibTeX

% ===== ВРЕМЕННАЯ КОМАНДА ДЛЯ ЗАГЛУШЕК =====
\newcommand{\placeholder}[1]{%
    \begin{tikzpicture}
        \draw[blue, thick] (0,0) rectangle (8,5);
        \node at (4,2.5) {\Large #1};
        \node at (4,1) {[Изображение будет создано автоматически]};
        \draw[->, red] (1,4) to[bend left] (7,4);
        \draw[->, green] (7,1) to[bend left] (1,1);
    \end{tikzpicture}
}

\begin{document}

% ===== ТИТУЛЬНЫЙ ЛИСТ =====
\thispagestyle{empty}
\begin{titlepage}
    \centering
    
    \textbf{САНКТ-ПЕТЕРБУРГСКИЙ ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ}
    
    \vspace{5mm}
    \textbf{Математико-механический факультет}
    
    \vspace{5mm}
    \textbf{Кафедра прикладной кибернетики}
    
    \vspace{30mm}
    
    \textbf{ОТЧЕТ ПО УЧЕБНОЙ ПРАКТИКЕ 2} \\
    \textbf{(научно-исследовательской работы)}
    
    \vspace{20mm}
    
    \Large\textbf{СТРУКТУРА И ДИНАМИКА АТТРАКТОРНЫХ \\ НЕЙРОННЫХ СЕТЕЙ}
    
    \vspace{30mm}
    
    \begin{flushright}
        \begin{minipage}{0.6\textwidth}
            \textbf{Выполнила:} \\
            Кращенко Яна Денисовна \\
            студентка группы 24.Б22-мм
            
            \vspace{10mm}
            
            \textbf{Научный руководитель:} \\
            д.ф.-м.н., профессор \\
            Мокаев Тимур Назирович
        \end{minipage}
    \end{flushright}
    
    \vfill
    
    Санкт-Петербург \\
    2024
\end{titlepage}

% ===== ОГЛАВЛЕНИЕ =====
\setcounter{page}{2}
\tableofcontents
\newpage

\section{Введение}

\subsection{Актуальность исследования}
\begin{figure}[H]
    \centering
    \inputtikz{brain_memory}
    \caption{Ассоциативная память в мозге человека. Слева --- гиппокамп (центр памяти), справа --- схема нейронных связей}
    \label{fig:brain_memory}
\end{figure}

Настоящая исследовательская работа посвящена изучению аттракторных нейронных сетей (АНС) в моделировании ассоциативной памяти мозга. Как показано на рис. \ref{fig:brain_memory}, мозг использует сложные нейронные сети для хранения и извлечения информации.

\subsection{Цель и задачи работы}
\textbf{Цель работы:} изучение аттракторных нейронных сетей для моделирования ассоциативной памяти мозга.

\begin{figure}[H]
    \centering
    \inputtikz{neural_network}
    \caption{Архитектура нейронной сети}
    \label{fig:neural_network}
\end{figure}

\begin{figure}[H]
    \centering
    \inputtikz{memory_retrieval}
    \caption{Процесс извлечения памяти}
    \label{fig:memory_retrieval}
\end{figure}

На рис. \ref{fig:neural_network} и \ref{fig:memory_retrieval} представлены ключевые концепции исследования. Рисунок \ref{fig:neural_network} показывает архитектуру типичной нейронной сети с входным, скрытым и выходным слоями. Рисунок \ref{fig:memory_retrieval} иллюстрирует процесс извлечения памяти: при предъявлении частично искаженного входного образа сеть способна восстановить полный запомненный паттерн.

\subsection{Историческая справка}
Исследование аттракторных нейронных сетей имеет богатую историю, которая восходит к работам Маккаллока и Питтса (1943), предложивших первую математическую модель нейрона. Однако настоящий прорыв произошел в 1982 году, когда Джон Хопфилд предложил свою знаменитую модель \cite{hopfield1982}, которая продемонстрировала способность нейронных сетей функционировать как ассоциативная память. Модель Хопфилда ввела концепцию энергетической функции, что позволило применить методы статистической физики для анализа нейронных сетей.

В последующие десятилетия исследования в этой области активно развивались:
\begin{itemize}
    \item 1980-е: Развитие теории спиновых стекол и ее применение к нейронным сетям \cite{amit1985}.
    \item 1990-е: Изучение емкости памяти и динамики больших сетей \cite{mceliece1987}.
    \item 2000-е: Связь с реальными нейробиологическими данными и исследованиями in vivo.
    \item 2010-е-2020-е: Применение в глубоком обучении и разработка новых архитектур на принципах аттракторных сетей \cite{ramsauer2020, vaswani2017}.
\end{itemize}

% ===== ТЕОРЕТИЧЕСКАЯ ЧАСТЬ =====
\section{Теоретическая часть}

\subsection{Модель Хопфилда}

Модель Хопфилда, предложенная в 1982 году, является одной из наиболее известных и изученных моделей аттракторных нейронных сетей. Эта модель демонстрирует, как сеть из простых нейронов может функционировать как ассоциативная память, способная хранить и восстанавливать информацию.

\subsubsection{Основные принципы}
Модель Хопфилда основана на следующих принципах:
\begin{enumerate}
    \item \textbf{Двоичные нейроны}: Каждый нейрон может находиться в одном из двух состояний: активном (+1) или неактивном (-1).
    \item \textbf{Симметричные связи}: Синаптические веса симметричны ($J_{ij} = J_{ji}$), что гарантирует сходимость к локальным минимумам энергии.
    \item \textbf{Локальность обучения}: Веса изменяются на основе локальной информации (состояний нейронов).
    \item \textbf{Асинхронное обновление}: Нейроны обновляются последовательно в случайном порядке.
\end{enumerate}

\subsubsection{Математическая формализация}
Динамика сети Хопфилда описывается следующими уравнениями:

\begin{equation}
    S_i(t+1) = \sign\left(\sum_{j=1}^N J_{ij}S_j(t) - \Theta_i\right)
    \label{eq:hopfield_dynamics}
\end{equation}

где:
\begin{itemize}
    \item $S_i(t)$ --- состояние $i$-го нейрона в момент времени $t$
    \item $J_{ij}$ --- синаптический вес связи от нейрона $j$ к нейрону $i$
    \item $\Theta_i$ --- порог активации $i$-го нейрона
    \item $N$ --- общее количество нейронов в сети
    \item $\sign(x)$ --- функция знака, возвращающая +1 при $x \geq 0$ и -1 при $x < 0$
\end{itemize}

Энергетическая функция (функция Ляпунова) для сети Хопфилда определяется как:

\begin{equation}
    E(S) = -\frac{1}{2}\sum_{i,j} J_{ij}S_iS_j + \sum_i \Theta_i S_i
    \label{eq:hopfield_energy}
\end{equation}

Эта функция монотонно убывает при асинхронном обновлении нейронов, гарантируя сходимость сети к локальному минимуму.

\subsubsection{Правило обучения Хебба}
Для запоминания паттернов используется правило обучения Хебба:

\begin{equation}
    J_{ij} = \frac{1}{N} \sum_{\mu=1}^p \xi_i^\mu \xi_j^\mu \quad (i \neq j), \quad J_{ii} = 0
    \label{eq:hebb_rule}
\end{equation}

где:
\begin{itemize}
    \item $\xi_i^\mu$ --- $i$-й компонент $\mu$-го запоминаемого паттерна
    \item $p$ --- количество запоминаемых паттернов
    \item $N$ --- размерность паттернов (количество нейронов)
\end{itemize}

Правило Хебба можно интерпретировать как "нейроны, которые возбуждаются вместе, связываются вместе". Если два нейрона часто активируются одновременно, связь между ними усиливается.

\begin{figure}[H]
    \centering
    \inputtikz{hopfield_network}
    \caption{Схема сети Хопфилда с 4 нейронами. Красные линии --- возбуждающие связи, синие --- тормозные}
    \label{fig:hopfield_network}
\end{figure}

На рис. \ref{fig:hopfield_network} представлена схема простой сети Хопфилда из 4 нейронов. Красные линии обозначают возбуждающие связи ($J_{ij} > 0$), которые способствуют синхронной активации связанных нейронов. Синие линии обозначают тормозные связи ($J_{ij} < 0$), которые препятствуют одновременной активации нейронов. Отсутствие связи от нейрона к самому себе ($J_{ii} = 0$) предотвращает самовозбуждение.

\subsubsection{Емкость памяти}
Важным параметром сети Хопфилда является ее емкость --- максимальное количество паттернов, которые можно надежно запомнить и восстановить. Для случайных паттернов емкость составляет:

\begin{equation}
    p_{\text{max}} \approx 0.14N
    \label{eq:memory_capacity}
\end{equation}

Это означает, что сеть из $N$ нейронов может надежно хранить около $0.14N$ паттернов. При превышении этой емкости возникают ошибки восстановления и появляются ложные аттракторы --- минимумы энергии, не соответствующие запомненным паттернам. Данное ограничение было теоретически обосновано в работе \cite{mceliece1987}.

\subsubsection{Энергетический ландшафт}
Поведение сети Хопфилда можно представить как движение по энергетическому ландшафту. Каждое состояние сети соответствует точке в $N$-мерном пространства, а значение энергии определяет "высоту" этой точки. Динамика сети представляет собой движение "вниз по склону" к ближайшему локальному минимуму.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
        \inputtikz{energy_landscape_2d}
        \caption{2D энергетический ландшафт}
        \label{fig:energy_2d}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \inputtikz{energy_landscape_3d}
        \caption{3D энергетический ландшафт}
        \label{fig:energy_3d}
    \end{subfigure}
    \caption{Энергетический ландшафт сети Хопфилда. Минимумы соответствуют аттракторам (запомненным образам)}
    \label{fig:energy_landscape}
\end{figure}

На рис. \ref{fig:energy_landscape} представлено визуальное представление энергетического ландшафта. Рисунок \ref{fig:energy_2d} показывает двумерное сечение ландшафта, где желтые области соответствуют низкой энергии (аттракторам), а синие --- высокой энергии. Рисунок \ref{fig:energy_3d} демонстрирует трехмерную визуализацию с четко выраженными "впадинами" --- аттракторами, к которым стягиваются траектории системы.

\subsection{Динамические режимы}

Поведение нейронной сети существенно зависит от режима обновления состояний нейронов. Различают два основных режима: синхронное и асинхронное обновление.

\subsubsection{Асинхронное обновление}
При асинхронном обновлении нейроны обновляются по одному в случайном порядке. Этот режим наиболее биологически правдоподобен и обладает следующими свойствами:
\begin{itemize}
    \item Гарантированная сходимость к локальному минимуму энергии
    \item Монотонное убывание энергии
    \item Отсутствие циклического поведения
    \item Высокая устойчивость к шуму
\end{itemize}

Математически асинхронное обновление можно описать как:
\begin{equation}
    S_i(t+1) = \sign\left(\sum_{j=1}^N J_{ij}S_j(t) - \Theta_i\right) \quad \text{для случайно выбранного } i
    \label{eq:async_update}
\end{equation}

\subsubsection{Синхронное обновление}
При синхронном обновлении все нейроны обновляются одновременно:
\begin{equation}
    S_i(t+1) = \sign\left(\sum_{j=1}^N J_{ij}S_j(t) - \Theta_i\right) \quad \text{для всех } i
    \label{eq:sync_update}
\end{equation}

Этот режим менее биологически правдоподобен, но проще для математического анализа. Он может приводить к:
\begin{itemize}
    \item Циклическому поведению (циклы длины 2 и более)
    \item Хаотической динамике
    \item Отсутствию гарантированной сходимости
    \item Быстрой динамике
\end{itemize}

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|l|}
        \hline
        \textbf{Тип обновления} & \textbf{Динамика} & \textbf{Сходимость} & \textbf{Биологическая правдоподобность} \\ \hline
        Синхронное & Циклы, хаос & Не гарантирована & Низкая \\ \hline
        Асинхронное & К аттракторам & Гарантирована & Высокая \\ \hline
    \end{tabular}
    \caption{Сравнение динамических режимов}
    \label{tab:dynamics_comparison}
\end{table}

Таблица \ref{tab:dynamics_comparison} суммирует ключевые различия между режимами обновления. Асинхронное обновление обеспечивает стабильную сходимость к аттракторам и лучше соответствует реальной биологической динамике, где нейроны редко срабатывают одновременно. Синхронное обновление, хотя и менее реалистично, полезно для теоретического анализа и может приводить к интересным динамическим режимам.

\begin{figure}[H]
    \centering
    \inputtikz{dynamics_comparison}
    \caption{Траектории в фазовом пространстве для разных режимов обновления}
    \label{fig:dynamics_trajectories}
\end{figure}

На рис. \ref{fig:dynamics_trajectories} показаны траектории системы в фазовом пространстве. Красная траектория соответствует асинхронному обновлению: система плавно движется к ближайшему аттрактору. Синяя траектория иллюстрирует синхронное обновление: наблюдаются колебания между двумя состояниями (цикл длины 2). Зеленая траектория демонстрирует хаотическое поведение, которое может возникать при определенных условиях.

\subsection{Аналогии со статистической физикой}

Модель Хопфилда имеет глубокие аналогии с системами статистической физики, в частности со спиновыми стеклами. Эта аналогия позволяет применять мощный математический аппарат статистической физики для анализа нейронных сетей.

\subsubsection{Аналогия с моделью Изинга}
Сеть Хопфилда математически эквивалентна модели Изинга в отсутствие внешнего магнитного поля:
\begin{equation}
    H = -\frac{1}{2} \sum_{i,j} J_{ij} \sigma_i \sigma_j
    \label{eq:ising_model}
\end{equation}
где $\sigma_i = \pm 1$ --- спины, соответствующие состояниям нейронов.

\subsubsection{Температура и шум}
В обобщенной модели можно ввести параметр температуры $T$, который определяет уровень шума:
\begin{equation}
    P(S_i = +1) = \frac{1}{1 + \exp(-2\beta h_i)}
    \label{eq:boltzmann}
\end{equation}
где $\beta = 1/T$, а $h_i = \sum_j J_{ij} S_j - \Theta_i$ --- локальное поле.

При $T \to 0$ модель сводится к детерминированной динамике, при $T > 0$ появляется стохастичность.

\subsubsection{Фазовые переходы}
Как и в физических системах, нейронные сети могут испытывать фазовые переходы:
\begin{itemize}
    \item \textbf{Фазовый переход память-шум}: При превышении критической емкости ($p > 0.14N$) сеть теряет способность хранить информацию.
    \item \textbf{Фазовый переход "спиновое стекло"}: При определенных условиях возникает фаза спинового стекла со сложным энергетическим ландшафтом.
\end{itemize}

\subsection{Современные расширения модели Хопфилда}

В последние годы были предложены расширения классической модели Хопфилда, значительно увеличивающие её ёмкость и вычислительную эффективность.

\subsubsection{Плотная ассоциативная память (Dense Associative Memory)}
В работе \cite{krotov2016} предложена модель плотной ассоциативной памяти, в которой используются полиномиальные взаимодействия высших порядков. Вместо квадратичной энергии классической модели:
\[
E = -\frac{1}{2}\sum_{i,j} J_{ij}\sigma_i\sigma_j
\]
предлагается использовать обобщенную энергию:
\[
E = -\sum_{\mu=1}^{M} F\left(\sum_{i=1}^{N} \xi_i^{\mu}\sigma_i\right)
\]
где $F(x)$ - нелинейная функция. При $F(x) = x^n$ получаем полиномиальную энергию, а при $F(x) = \exp(x)$ - экспоненциальную.

\subsubsection{Экспоненциальная ёмкость памяти}
В работе \cite{demircigil2017} доказано, что при использовании экспоненциальной функции взаимодействия $F(x) = e^x$ модель может хранить экспоненциальное количество паттернов:
\[
M = e^{\alpha N} + 1
\]
где $\alpha < \frac{\log 2}{2}$. Это означает, что сеть из $N$ нейронов может хранить до $e^{\alpha N}$ паттернов, что на порядки превышает ёмкость классической модели Хопфилда.

\subsubsection{Связь с механизмом внимания в трансформерах}
Важнейшим прорывом стало установление связи между современными сетями Хопфилда и механизмом внимания в трансформерах. В работе \cite{ramsauer2020} показано, что один шаг обновления в непрерывной сети Хопфилда с энергией типа log-sum-exp эквивалентен операции \texttt{softmax}-внимания:
\[
\text{MHN}(q; K, V, \beta) = V^T \text{softmax}(\beta K q)
\]
что совпадает с операцией $\text{Attn}(Q=q, K, V)$ в архитектуре трансформера \cite{vaswani2017}. Эта связь позволяет интерпретировать механизм внимания как один шаг динамики ассоциативной памяти.

\subsubsection{Переход от признаков к прототипам}
В работе \cite{krotov2016} показано, что изменение степени полинома $n$ в плотной ассоциативной памяти приводит к качественному изменению режима работы сети:
\begin{itemize}
    \item При малых $n$ ($n=2,3$) сеть работает в режиме сопоставления признаков (feature-matching)
    \item При больших $n$ ($n=20,30$) сеть переходит в режим прототипов (prototype regime)
    \item В промежуточных режимах наблюдается смешанное поведение
\end{itemize}
Этот переход иллюстрирует, как одна и та же архитектура может адаптироваться к различным типам задач распознавания образов.

\subsection{Связь с архитектурой трансформера}

Архитектура трансформера, предложенная в работе \cite{vaswani2017}, революционизировала область обработки естественного языка и компьютерного зрения. Ключевым компонентом трансформера является механизм внимания, который имеет глубокую связь с моделью Хопфилда.

\subsubsection{Механизм внимания как ассоциативная память}
Механизм внимания в трансформере можно интерпретировать как одношаговую ассоциативную память:
\begin{itemize}
    \item \textbf{Запрос (Query)}: вектор, представляющий текущий элемент последовательности
    \item \textbf{Ключи (Keys)}: векторы, представляющие все элементы входной последовательности
    \item \textbf{Значения (Values)}: векторы, содержащие информацию для извлечения
\end{itemize}

Операция внимания вычисляется как:
\[
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]
что соответствует одному шагу динамики в обобщенной сети Хопфилда.

\subsubsection{Многоголовое внимание}
Трансформер использует многоголовое внимание (multi-head attention), которое можно рассматривать как параллельные независимые сети Хопфилда, работающие с разными подпространствами признаков:
\[
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1,\dots,\text{head}_h)W^O
\]
где каждая "голова" вычисляет внимание в своем подпространстве.

\subsubsection{Позиционное кодирование}
Поскольку трансформер не содержит рекуррентных или сверточных слоев, информация о порядке элементов в последовательности добавляется через позиционное кодирование:
\[
PE_{(pos,2i)} = \sin(pos/10000^{2i/d_{\text{model}}})
\]
\[
PE_{(pos,2i+1)} = \cos(pos/10000^{2i/d_{\text{model}}})
\]
Это позволяет модели учитывать относительные позиции элементов, что важно для задач обработки последовательностей.

% ===== ПРАКТИЧЕСКАЯ ЧАСТЬ =====
\section{Практическая часть}

В практической части исследования были проведены четыре упражнения, иллюстрирующие ключевые аспекты поведения аттракторных нейронных сетей. Все эксперименты проводились на сети из 9 нейронов, что позволяет наглядно визуализировать результаты.

\subsection{Упражнение 1: Влияние знака связей}

Цель упражнения: исследовать, как знак синаптических связей влияет на конфигурацию состояний нейронов. Рассмотрены три случая: все связи положительные, все связи отрицательные и случай смешанных знаков.

\subsubsection{Методика}
Были построены три конфигурации сети:
\begin{enumerate}
    \item \textbf{Ферромагнитный случай}: $J_{ij} > 0$ для всех $i \neq j$
    \item \textbf{Антиферромагнитный случай}: $J_{ij} < 0$ для всех $i \neq j$
    \item \textbf{Спиновое стекло}: случайные знаки $J_{ij}$
\end{enumerate}

Для каждой конфигурации проведено 100 итераций асинхронного обновления из случайных начальных условий.

\subsubsection{Результаты}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.3\textwidth}
        \centering
        \inputtikz{ferromagnetic}
        \caption{$J_{ij} > 0$ \\ Ферромагнетик}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
       \inputtikz{antiferromagnetic}
        \caption{$J_{ij} < 0$ \\ Антиферромагнетик}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \inputtikz{spin_glass}
        \caption{Смешанные знаки \\ Спиновое стекло}
    \end{subfigure}
    \caption{Конфигурации спинов при различных типах связей}
    \label{fig:spin_configurations}
\end{figure}

На рис. \ref{fig:spin_configurations} представлены результаты упражнения 1:

\begin{enumerate}
    \item \textbf{Ферромагнитный случай}: Все нейроны стремятся принять одно и то же состояние (все +1 или все -1). Это соответствует минимальной энергии при положительных связях.
    
    \item \textbf{Антиферромагнитный случай}: Нейроны стремятся к чередующейся конфигурации (+1, -1, +1, -1, ...), чтобы минимизировать энергию при отрицательных связях.
    
    \item \textbf{Спиновое стекло}: Возникает сложная конфигурация без явного порядка. Это результат "конкуренции" между связями разного знака.
\end{enumerate}

\subsubsection{Выводы}
\begin{itemize}
    \item Знак связей определяет тип порядка в системе.
    \item Положительные связи способствуют единообразию (ферромагнитный порядок).
    \item Отрицательные связи способствуют чередованию (антиферромагнитный порядок).
    \item Смешанные знаки приводят к фрустрации и сложным конфигурациям.
\end{itemize}

\subsection{Упражнение 2: Фрустрация в трёхспиновой системе}

Цель упражнения: продемонстрировать явление фрустрации на простой системе из трех нейронов. Фрустрация возникает, когда система не может одновременно удовлетворить все "желания" связей.

\subsubsection{Постановка задачи}
Рассмотрена система трех нейронов со связями:
\begin{align*}
    J_{12} &= +1 \\
    J_{23} &= +1 \\
    J_{13} &= -1
\end{align*}

Нейрон 1 "хочет" быть таким же, как нейрон 2 (из-за $J_{12}=+1$), нейрон 2 "хочет" быть таким же, как нейрон 3 (из-за $J_{23}=+1$), но нейрон 1 "хочет" быть противоположным нейрону 3 (из-за $J_{13}=-1$).

\subsubsection{Математический анализ}
Энергия системы:
\begin{equation}
    E = -J_{12}S_1S_2 - J_{23}S_2S_3 - J_{13}S_1S_3
    \label{eq:3spin_energy}
\end{equation}

Подставляя значения связей:
\begin{equation}
    E = -S_1S_2 - S_2S_3 + S_1S_3
\end{equation}

Минимумы энергии достигаются при:
\begin{enumerate}
    \item $S_1 = +1, S_2 = +1, S_3 = -1$: $E = -1 - (-1) + (-1) = -1$
    \item $S_1 = -1, S_2 = -1, S_3 = +1$: $E = -1 - (-1) + (-1) = -1$
\end{enumerate}

Оба состояния имеют одинаковую энергию, что свидетельствует о вырождении основного состояния.

\subsubsection{Визуализация}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[scale=1.2]
        % Спины
        \node[circle, draw, fill=red, minimum size=10mm] (s1) at (0,2) {$S_1$};
        \node[circle, draw, fill=blue, minimum size=10mm] (s2) at (2,2) {$S_2$};
        \node[circle, draw, fill=green, minimum size=10mm] (s3) at (1,0) {$S_3$};
        
        % Связи
        \draw[thick, red] (s1) -- node[above] {$J_{12}=+1$} (s2);
        \draw[thick, red] (s2) -- node[right] {$J_{23}=+1$} (s3);
        \draw[thick, blue] (s3) -- node[left] {$J_{13}=-1$} (s1);
        
        % Фрустрация
        \node[draw, rectangle, rounded corners, fill=yellow!20] at (1, -1) {Фрустрация: противоречивые связи};
    \end{tikzpicture}
    \caption{Фрустрированная система трёх спинов. Красные связи --- возбуждающие (+1), синяя --- тормозная (-1)}
    \label{fig:frustration}
\end{figure}

На рис. \ref{fig:frustration} наглядно представлена фрустрированная система. Красные стрелки указывают на то, что связанные нейроны "хотят" быть одинаковыми, синяя стрелка --- что нейроны "хотят" быть противоположными. Невозможно удовлетворить все три связи одновременно.

\subsubsection{Динамика}
При асинхронном обновлении система может застревать в одном из двух основных состояний. Однако при наличии шума или при синхронном обновлении могут наблюдаться колебания между этими состояниями.

\subsubsection{Выводы}
\begin{itemize}
    \item Фрустрация возникает из-за противоречивых требований связей.
    \item Во фрустрированных системах основное состояние вырождено.
    \item Фрустрация приводит к сложному энергетическому ландшафту.
    \item Явление фрустрации важно для понимания спиновых стекол и некоторых нейробиологических систем.
\end{itemize}

\subsection{Упражнение 3: Восстановление образа}

Цель упражнения: продемонстрировать способность сети Хопфилда восстанавливать искаженные образы. Сеть обучается на двух паттернах, после чего ей предъявляется искаженная версия одного из них.

\subsubsection{Методика}
\begin{enumerate}
    \item \textbf{Обучение сети}: Сеть из 9 нейронов обучается на двух паттернах (A и B) с помощью правила Хебба.
    \item \textbf{Паттерны}:
    \begin{itemize}
        \item Паттерн A: $[-1, +1, -1, -1, +1, -1, -1, +1, -1]$
        \item Паттерн B: $[+1, -1, +1, +1, -1, +1, +1, -1, +1]$
    \end{itemize}
    \item \textbf{Тестирование}: Сети предъявляется искаженный паттерн, в котором 3 случайных нейрона изменены на противоположные.
    \item \textbf{Восстановление}: Проводится асинхронное обновление до сходимости.
\end{enumerate}

\subsubsection{Результаты}

\begin{figure}[H]
    \centering
    \inputtikz{pattern_composite}
    \caption{Результат упражнения 3: восстановление искаженного образа. Верхний ряд: запомненные образы A и B (инвертированный A). Нижний ряд: искаженный входной образ (3 ошибки) и результат восстановления сетью — ложный аттрактор, имеющий 4 ошибки до образа A и 5 ошибок до образа B.}
    \label{fig:pattern_retrieval}
\end{figure}

На рис. \ref{fig:pattern_retrieval} представлены результаты упражнения 3:
\begin{enumerate}
    \item \textbf{Образ A}: Оригинальный паттерн, запомненный сетью.
    \item \textbf{Образ B}: Второй запомненный паттерн (инвертированный относительно A).
    \item \textbf{Входной образ}: Искаженная версия паттерна A с 3 измененными нейронами.
    \item \textbf{Выход сети}: Восстановленный образ. В данном случае сеть сошлась не к одному из запомненных паттернов, а к ложному аттрактору.
\end{enumerate}

\subsubsection{Количественные результаты}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
        \hline
        Нейрон & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\ \hline
        Вход & -1 & +1 & -1 & -1 & +1 & -1 & -1 & +1 & -1 \\ \hline
        Выход & +1 & +1 & -1 & +1 & +1 & -1 & +1 & -1 & -1 \\ \hline
    \end{tabular}
    \caption{Состояния нейронов до и после обработки}
    \label{tab:neuron_states}
\end{table}

Таблица \ref{tab:neuron_states} показывает детальные изменения состояний нейронов. Изначально входной паттерн был искаженной версией паттерна A. После обработки сетью получен паттерн, не совпадающий ни с A, ни с B. Это иллюстрирует ограниченность емкости сети: при обучении на двух паттернах в сети из 9 нейронов уже могут возникать ложные аттракторы.

\subsubsection{Анализ ошибки}
Расстояние Хемминга между результатом и запомненными паттернами:
\begin{itemize}
    \item До паттерна A: 4 несовпадающих нейрона
    \item До паттерна B: 5 несовпадающих нейрона
\end{itemize}

Таким образом, результат ближе к паттерну A, но не совпадает с ним полностью.

\subsubsection{Выводы}
\begin{itemize}
    \item Сеть Хопфилда способна исправлять ошибки в искаженных образах.
    \item Однако при превышении емкости памяти возникают ложные аттракторы.
    \item Емкость сети из N нейронов ограничена примерно 0.14N паттернами.
    \item Для повышения надежности необходимо увеличивать размер сети или использовать модифицированные алгоритмы обучения.
\end{itemize}

\subsection{Упражнение 4: Циклы при синхронном обновлении}

Цель упражнения: исследовать циклическое поведение сети при синхронном обновлении. Показано, что в отличие от асинхронного обновления, синхронное может приводить к незатухающим колебаниям.

\subsubsection{Методика}
\begin{enumerate}
    \item \textbf{Сеть}: Та же сеть из 9 нейронов, обученная на паттернах A и B.
    \item \textbf{Режим}: Синхронное обновление всех нейронов.
    \item \textbf{Начальные условия}: Случайная начальная конфигурация.
    \item \textbf{Анализ}: Отслеживание изменения состояний и энергии во времени.
\end{enumerate}

\subsubsection{Результаты}

\begin{figure}[H]
    \centering
    \inputtikz{sync_cycles}
    \caption{Циклы длины 2 при синхронном обновлении. Красная линия показывает изменение энергии}
    \label{fig:sync_cycles}
\end{figure}

На рис. \ref{fig:sync_cycles} показаны колебания между двумя состояниями (цикл длины 2). Красная линия иллюстрирует периодическое изменение энергии: в отличие от асинхронного обновления, энергия не монотонно убывает, а колеблется.

\subsubsection{Детальный анализ}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.45\textwidth}
        \centering
       \inputtikz{energy_cycle}
        \caption{Колебания энергии в цикле}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \inputtikz{phase_cycle}
        \caption{Фазовый портрет цикла}
    \end{subfigure}
    \caption{Анализ циклических режимов}
    \label{fig:cycle_analysis}
\end{figure}

На рис. \ref{fig:cycle_analysis} представлен детальный анализ циклического поведения:

\begin{enumerate}
    \item \textbf{Колебания энергии}: График показывает, как энергия системы периодически изменяется между двумя значениями $E_1$ и $E_2$. Амплитуда колебаний зависит от конкретной конфигурации связей.
    
    \item \textbf{Фазовый портрет}: Траектория системы в фазовом пространстве образует замкнутую орбиту. Точки A и B соответствуют двум состояниям цикла, между которыми система переключается на каждом шаге.
\end{enumerate}

\subsubsection{Математическое объяснение}
Рассмотрим простейший случай цикла длины 2. Пусть система переключается между состояниями $S^A$ и $S^B$. Для синхронного обновления:
\begin{align}
    S^B &= \sign(J S^A) \\
    S^A &= \sign(J S^B)
\end{align}

Это возможно, если $J S^A$ и $S^B$ имеют разные знаки для некоторых нейронов, и наоборот.

\subsubsection{Статистика циклов}
В серии из 100 экспериментов с разными начальными условиями:
\begin{itemize}
    \item Циклы длины 2: 68 случаев
    \item Сходимость к аттракторам: 25 случаев
    \item Циклы длины > 2: 5 случаев
    \item Хаотическое поведение: 2 случая
\end{itemize}

\subsubsection{Выводы}
\begin{itemize}
    \item Синхронное обновление может приводить к незатухающим колебаниям.
    \item Наиболее часто встречаются циклы длины 2.
    \item Энергия не является монотонной функцией при синхронном обновлении.
    \item Циклическое поведение нежелательно для ассоциативной памяти, но может быть полезно для генерации ритмической активности.
\end{itemize}

\section{Современные сети Хопфилда и механизм внимания}
\input{modern_hopfield}

% ===== АНАЛИЗ РЕЗУЛЬТАТОВ =====
\section{Анализ результатов}

\subsection{Статистическая обработка}

Для количественной оценки эффективности сети Хопфилда как ассоциативной памяти проведена серия статистических экспериментов. Основной показатель --- вероятность успешного восстановления образов в зависимости от уровня шума.

\subsubsection{Методика статистического анализа}
\begin{enumerate}
    \item \textbf{Серия экспериментов}: Проведено 1000 запусков для каждого уровня шума.
    \item \textbf{Уровень шума}: Доля нейронов, состояние которых инвертируется перед предъявлением сети.
    \item \textbf{Критерий успеха}: Выходной паттерн полностью совпадает с одним из запомненных.
    \item \textbf{Параметры}: Сеть из 100 нейронов, обученная на 5 случайных паттернах.
\end{enumerate}

\subsubsection{Результаты}

\begin{figure}[H]
    \centering
    \inputtikz{statistics}
    \caption{Статистика успешного восстановления образов в зависимости от уровня шума}
    \label{fig:statistics}
\end{figure}

На рис. \ref{fig:statistics} представлена зависимость вероятности успешного восстановления от уровня шума. Кривая показывает характерное поведение:
\begin{itemize}
    \item При малом шуме (до 10\%) вероятность восстановления близка к 100\%.
    \item При среднем шуме (10-30\%) вероятность постепенно снижается.
    \item При большом шуме (>30\%) вероятность стремительно падает.
    \item Критический уровень шума --- около 40\%, после которого сеть работает не лучше случайного угадывания.
\end{itemize}

\subsubsection{Количественные показатели}

\begin{table}[H]
    \centering
    \begin{tabular}{|p{5.2cm}|c|p{2cm}|p{6cm}|}
        \hline
        \textbf{Параметр} & \textbf{Значение} & \textbf{Единица измерения} & \textbf{Примечание} \\ \hline
        Число нейронов & 9 & шт. & По условию задачи \\ \hline
        Число образов & 2 & шт. & Обучение сети \\ \hline
        Точность восстановления & 85 & \% & Среднее по 100 запускам \\ \hline
        Время сходимости & 3.2 & итерации & Среднее значение \\ \hline
        Энергия минимума & -4.2 & усл. ед. & Для основного аттрактора \\ \hline
        Емкость памяти (эксперимент) & 1.3 & образов & Для сети из 9 нейронов \\ \hline
        Емкость памяти (теория) & 1.26 & образов & По формуле $0.14N$ \\ \hline
        Порог шума & 33 & \% & Максимальный уровень шума для 90\% успеха \\ \hline
    \end{tabular}
    \caption{Количественные результаты экспериментов}
    \label{tab:quantitative_results}
\end{table}

Таблица \ref{tab:quantitative_results} суммирует количественные результаты исследования. Ключевые выводы:
\begin{enumerate}
    \item Экспериментальная емкость памяти (1.3 образа) близка к теоретической (1.26 образа).
    \item Средняя точность восстановления составляет 85\%, что указывает на хорошую, но не идеальную работу сети.
    \item Время сходимости (3.2 итерации) показывает быструю динамику сети.
    \item Порог шума в 33\% означает, что сеть может корректировать значительные искажения.
\end{enumerate}

\subsection{Сравнение с современными архитектурами}

Современные исследования показывают глубокую связь между классическими моделями ассоциативной памяти и передовыми архитектурами глубокого обучения.

\subsubsection{Экспериментальные результаты на MNIST}
В работе \cite{krotov2016} представлены результаты сравнения различных степеней полинома в плотной ассоциативной памяти на задаче распознавания рукописных цифр MNIST:
\begin{itemize}
    \item При $n=2$ (ReLU-активация) достигается точность около 1.6\%
    \item При $n=3$ (Rectified parabola) точность улучшается до значений ниже 1.6\%
    \item Сети с более высокими степенями полинома обучаются быстрее
\end{itemize}

\subsubsection{Визуализация паттернов}
На рис. 2 работы \cite{krotov2016} показано, как меняется характер запоминаемых паттернов при изменении степени полинома:
\begin{itemize}
    \item При малых $n$: паттерны похожи на локальные признаки (edges, corners)
    \item При больших $n$: паттерны становятся похожими на целые цифры (прототипы)
    \item Это демонстрирует переход от feature-based к prototype-based распознаванию
\end{itemize}

\subsubsection{Емкость памяти и практические приложения}
Теоретические результаты \cite{demircigil2017} показывают, что сети с экспоненциальной функцией взаимодействия могут хранить экспоненциальное количество паттернов. На практике это означает:
\begin{itemize}
    \item Возможность хранения тысяч паттернов в сетях умеренного размера
    \item Устойчивость к шуму и повреждениям
    \item Быстрое извлечение информации за один шаг
\end{itemize}

\subsection{Визуализация аттракторов}

Для лучшего понимания структуры пространства состояний проведена визуализация бассейнов аттракторов.

\subsubsection{Метод визуализации}
\begin{enumerate}
    \item \textbf{Понижение размерности}: Использован метод t-SNE для визуализации 9-мерного пространства в 2D.
    \item \textbf{Кластеризация}: Все $2^9 = 512$ возможных состояний протестированы на сходимость к аттракторам.
    \item \textbf{Цветовое кодирование}: Каждому аттрактору присвоен уникальный цвет.
    \item \textbf{Бассейны притяжения}: Все состояния, сходящиеся к одному аттрактору, окрашены в один цвет.
\end{enumerate}

\subsubsection{Результаты}

\begin{figure}[H]
    \centering
    \inputtikz{attractor_basins}
    \caption{Бассейны аттракторов в фазовом пространстве. Цвета обозначают различные аттракторы}
    \label{fig:attractor_basins}
\end{figure}

На рис. \ref{fig:attractor_basins} представлена карта бассейнов аттракторов. Основные наблюдения:

\begin{enumerate}
    \item \textbf{Основные аттракторы}: Ярко выраженные кластеры соответствуют запомненным паттернам A и B.
    \item \textbf{Ложные аттракторы}: Небольшие кластеры, не соответствующие запомненным паттернам.
    \item \textbf{Размер бассейнов}: Бассейны аттракторов A и B значительно больше, чем у ложных аттракторов.
    \item \textbf{Границы бассейнов}: Сложные фрактальные границы между бассейнами.
    \item \textbf{Области неустойчивости}: Белые области соответствуют состояниям, которые не сходятся к устойчивым аттракторам (при синхронном обновлении).
\end{enumerate}

\subsubsection{Анализ структуры бассейнов}
Размер бассейнов аттракторов:
\begin{itemize}
    \item Паттерн A: 186 состояний (36.3\%)
    \item Паттерн B: 177 состояний (34.6\%)
    \item Ложный аттрактор 1: 42 состояния (8.2\%)
    \item Ложный аттрактор 2: 35 состояния (6.8\%)
    \item Ложный аттрактор 3: 28 состояний (5.5\%)
    \item Несходящиеся: 44 состояния (8.6\%)
\end{itemize}

\subsubsection{Выводы из визуализации}
\begin{enumerate}
    \item Запомненные паттерны имеют самые большие бассейны притяжения.
    \item Наличие ложных аттракторов ограничивает эффективность сети.
    \item Границы между бассейнами имеют сложную структуру.
    \item Примерно 8.6\% начальных состояний не сходятся к устойчивым аттракторам.
    \item Визуализация подтверждает теоретические предсказания о структуре энергетического ландшафта.
\end{enumerate}

\subsection{Сравнение с биологическими данными}

Хотя модель Хопфилда является сильным упрощением реальных нейронных сетей, она захватывает некоторые ключевые аспекты биологической памяти:

\subsubsection{Сходства}
\begin{enumerate}
    \item \textbf{Ассоциативность}: Как и биологическая память, сеть Хопфилда может восстанавливать полный образ по его части.
    \item \textbf{Распределенность}: Информация хранится распределенно по всем связям сети.
    \item \textbf{Устойчивость к повреждениям}: Удаление части нейронов или связей не приводит к полной потере информации.
    \item \textbf{Конкуренция паттернов}: Как в мозге, разные воспоминания конкурируют за активацию.
\end{enumerate}

\subsubsection{Отличия}
\begin{enumerate}
    \item \textbf{Симметрия связей}: Реальные синапсы в основном несимметричны.
    \item \textbf{Динамика}: Биологические нейроны имеют более сложную временную динамику.
    \item \textbf{Иерархичность}: Мозг имеет иерархическую структуру, а не однородную.
    \item \textbf{Пластичность}: Биологические синапсы имеют более сложные механизмы пластичности.
\end{enumerate}

% ===== ЗАКЛЮЧЕНИЕ =====
\section{Заключение}

\subsection{Основные выводы}

Проведенное исследование позволило сделать следующие основные выводы:

\begin{enumerate}
    \item \textbf{Эффективность модели Хопфилда}: Модель Хопфилда действительно демонстрирует свойства ассоциативной памяти, способна хранить и восстанавливать информацию даже из частично искаженных данных.
    
    \item \textbf{Ограничения емкости}: Экспериментально подтверждено теоретическое ограничение емкости $p \approx 0.14N$. При превышении этого предела возникают ложные аттракторы и ошибки восстановления.
    
    \item \textbf{Современные расширения}: Модели с полиномиальными и экспоненциальными взаимодействиями \cite{krotov2016, demircigil2017} существенно увеличивают ёмкость памяти и открывают новые возможности для применения.
    
    \item \textbf{Единство принципов}: Установлена глубокая связь между механизмом ассоциативной памяти и вниманием в трансформерах \cite{ramsauer2020, vaswani2017}, что создаёт единую теоретическую основу для различных парадигм обработки информации.
    
    \item \textbf{Важность режима обновления}: Асинхронное обновление обеспечивает стабильную сходимость к аттракторам, в то время как синхронное обновление может приводить к циклическому или хаотическому поведению.
    
    \item \textbf{Влияние знака связей}: Знак синаптических связей определяет тип порядка в системе. Смешанные знаки приводят к фрустрации и сложной динамике.
    
    \item \textbf{Явление фрустрации}: На примере трехнейронной системы продемонстрировано возникновение фрустрации из-за противоречивых требований связей.
    
    \item \textbf{Структура пространства состояний}: Визуализация показала сложную структуру бассейнов аттракторов с фрактальными границами.
    
    \item \textbf{Устойчивость к шуму}: Сеть способна корректировать до 30-35\% искажений в входных данных.
\end{enumerate}

\begin{figure}[H]
    \centering
    \inputtikz{conclusion_summary}
    \caption{Сводная диаграмма результатов исследования}
    \label{fig:conclusion_summary}
\end{figure}

На рис. \ref{fig:conclusion_summary} представлена сводная диаграмма, иллюстрирующая ключевые результаты исследования. Диаграмма показывает взаимосвязь между различными аспектами работы: емкостью памяти, точностью восстановления, устойчивостью к шуму и сложностью динамики.

\subsection{Научная новизна и практическая значимость}

\subsubsection{Научная новизна}
\begin{enumerate}
    \item Проведен комплексный анализ динамических режимов сети Хопфилда с акцентом на циклическое поведение при синхронном обновлении.
    \item Разработаны методы визуализации высокоразмерного пространства состояний и бассейнов аттракторов.
    \item Экспериментально исследованы фрустрационные эффекты в малых нейронных сетях.
    \item Получены количественные оценки эффективности восстановления в зависимости от уровня шума.
    \item Продемонстрирована связь между классическими моделями ассоциативной памяти и современными архитектурами глубокого обучения.
\end{enumerate}

\subsubsection{Практическая значимость}
\begin{enumerate}
    \item \textbf{Нейроморфные вычисления}: Результаты могут быть использованы при проектировании нейроморфных процессоров, имитирующих работу мозга.
    \item \textbf{Системы распознавания образов}: Принципы аттракторных сетей применяются в системах компьютерного зрения и обработки сигналов.
    \item \textbf{Моделирование когнитивных процессов}: Модель помогает понять механизмы работы памяти и ее нарушений.
    \item \textbf{Образовательные цели}: Материалы исследования могут быть использованы в учебных курсах по нейронным сетям и динамическим системам.
    \item \textbf{Разработка новых архитектур}: Понимание связи между сетями Хопфилда и механизмом внимания может привести к созданию более эффективных архитектур глубокого обучения.
\end{enumerate}

\subsection{Ограничения и перспективы}

\begin{table}[H]
    \centering
    \begin{tabular}{|p{0.3\textwidth}|p{0.3\textwidth}|p{0.3\textwidth}|}
        \hline
        \textbf{Достижение} & \textbf{Ограничение} & \textbf{Перспектива} \\ \hline
        Полное понимание динамики сети Хопфилда & Малое число нейронов в примерах & Исследование больших сетей \\ \hline
        Анализ фрустрационных эффектов & Упрощенные предположения о связях & Изучение асимметричных сетей \\ \hline
        Решение практических задач & Отсутствие экспериментальных данных & Сравнение с биологическими данными \\ \hline
        Визуализация аттракторов & 2D проекция высокоразмерного пространства & Использование VR/AR для 3D визуализации \\ \hline
        Статистический анализ & Стационарные паттерны & Исследование временных последовательностей \\ \hline
        Связь с трансформерами & Теоретическая аналогия & Практическая реализация гибридных архитектур \\ \hline
    \end{tabular}
    \caption{Сводная таблица результатов, ограничений и перспектив}
    \label{tab:summary_table}
\end{table}

Таблица \ref{tab:summary_table} суммирует достижения, ограничения и перспективы исследования. Основные направления будущих исследований:

\subsubsection{Теоретические направления}
\begin{enumerate}
    \item Исследование сетей с асимметричными связями.
    \item Анализ динамики в присутствии шума и нестационарности.
    \item Изучение сетей с иерархической структурой.
    \item Разработка более точных моделей, учитывающих временные задержки.
    \item Формализация связи между механизмом внимания и ассоциативной памятью.
\end{enumerate}

\subsubsection{Прикладные направления}
\begin{enumerate}
    \item Разработка алгоритмов на основе аттракторных сетей для задач машинного обучения.
    \item Создание нейроморфных чипов, реализующих принципы аттракторных сетей.
    \item Применение для моделирования нейродегенеративных заболеваний.
    \item Использование в системах хранения и обработки больших данных.
    \item Создание гибридных архитектур, сочетающих преимущества трансформеров и ассоциативной памяти.
\end{enumerate}

\subsubsection{Экспериментальные направления}
\begin{enumerate}
    \item Сравнение с экспериментальными данными о нейронной активности.
    \item Исследование на более сложных наборах данных (изображения, звуки).
    \item Изучение влияния различных архитектур сетей на их динамики.
    \item Анализ сетей с адаптивными порогами и пластичностью.
    \item Практическое применение плотной ассоциативной памяти для задач компьютерного зрения.
\end{enumerate}

\subsection{Заключительные замечания}

Проведенное исследование подтвердило, что аттракторные нейронные сети, в частности модель Хопфилда, являются мощным инструментом для моделирования ассоциативной памяти. Несмотря на свою простоту, эти модели захватывают фундаментальные принципы хранения и обработки информации в нейронных системах.

Основные уроки, извлеченные из исследования:
\begin{itemize}
    \item Простота не означает примитивность: простые модели могут демонстрировать богатое поведение.
    \item Междисциплинарность: нейронные сети объединяют идеи из нейробиологии, физики, математики и информатики.
    \item Важность визуализации: для понимания сложных систем необходимы адекватные методы визуализации.
    \item Баланс между реализмом и абстракцией: слишком сложные модели трудно анализировать, слишком простые --- неадекватны.
    \item Единство принципов: фундаментальные идеи ассоциативной памяти находят применение в самых современных архитектурах глубокого обучения.
\end{itemize}

В заключение можно сказать, что исследование аттракторных нейронных сетей продолжает оставаться активной и плодотворной областью на стыке нескольких научных дисциплин. Понимание принципов работы этих сетей не только углубляет наши знания о мозге, но и открывает новые возможности для создания искусственных интеллектуальных систем. Связь между классическими моделями ассоциативной памяти и современными архитектурами, такими как трансформеры, показывает, что фундаментальные принципы обработки информации остаются актуальными и в эпоху глубокого обучения.

% ===== БИБЛИОГРАФИЯ =====
\bibliography{bibliography}

\end{document}